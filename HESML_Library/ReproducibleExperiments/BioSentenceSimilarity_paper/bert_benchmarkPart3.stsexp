<?xml version="1.0" encoding="UTF-8"?>
<!-- edited with XMLSPY v5 U (http://www.xmlspy.com) by Ramon Yepes (Investronica) -->
<SentenceSimilarityExperiments xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="../SentenceSimilarityExperiments.xsd">
	<SingleDatasetSentenceSimilarityValuesExperiment>
		<OutputFilename>BioSentenceSimFinalRawOutputFiles/raw_similarity_BIOSSES.csv</OutputFilename>
		<DatasetDirectory>../SentenceSimDatasets</DatasetDirectory>
		<DatasetFilename>BIOSSESNormalized.tsv</DatasetFilename>
		<SentenceSimilarityMeasures>

			<BertEmbeddingModelMeasure>
				<Label>oubiobert-base-uncased</Label>
				<Method>BERTEmbeddingModel</Method>
				<MLPythonLibrary>Pytorch</MLPythonLibrary>
				<PretrainedModelDirectory></PretrainedModelDirectory>
				<PretrainedModelName>seiya/oubiobert-base-uncased</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/PytorchExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/PytorchExperiments/venv/bin/python</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>Biosses2017StopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>BIOSSES</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/BERTPretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>oubiobert-base-uncased</PretrainedModelFilename>
					<NERType>None</NERType>
				</WordProcessing>
			</BertEmbeddingModelMeasure>

			<BertEmbeddingModelMeasure>
				<Label>PubMedBERT-base-uncased-abstract-fulltext</Label>
				<Method>BERTEmbeddingModel</Method>
				<MLPythonLibrary>Pytorch</MLPythonLibrary>
				<PretrainedModelDirectory></PretrainedModelDirectory>
				<PretrainedModelName>microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/PytorchExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/PytorchExperiments/venv/bin/python</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>Biosses2017StopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>BIOSSES</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/BERTPretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>PubMedBERT-base-uncased-abstract-fulltext</PretrainedModelFilename>
					<NERType>None</NERType>
				</WordProcessing>
			</BertEmbeddingModelMeasure>

			<BertEmbeddingModelMeasure>
				<Label>PubMedBERT-base-uncased-abstract</Label>
				<Method>BERTEmbeddingModel</Method>
				<MLPythonLibrary>Pytorch</MLPythonLibrary>
				<PretrainedModelDirectory></PretrainedModelDirectory>
				<PretrainedModelName>microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/PytorchExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/PytorchExperiments/venv/bin/python</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>Biosses2017StopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>BIOSSES</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/BERTPretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>PubMedBERT-base-uncased-abstract</PretrainedModelFilename>
					<NERType>None</NERType>
				</WordProcessing>
			</BertEmbeddingModelMeasure>

			<BertEmbeddingModelMeasure>
				<Label>scibert_scivocab_uncased</Label>
				<Method>BERTEmbeddingModel</Method>
				<MLPythonLibrary>Tensorflow</MLPythonLibrary>
				<Pooling>REDUCE_MEAN</Pooling>
				<PoolingLayers>-2</PoolingLayers>
				<PretrainedModelDirectory>../BERTExperiments/BERTPretrainedModels/</PretrainedModelDirectory>
				<PretrainedModelName>scibert_scivocab_uncased</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>Biosses2017StopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>BIOSSES</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/BERTPretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>scibert_scivocab_uncased</PretrainedModelFilename>
					<NERType>None</NERType>
				</WordProcessing>
			</BertEmbeddingModelMeasure>

			

			<BertEmbeddingModelMeasure>
				<Label>biobert_v1.1_pubmed</Label>
				<Method>BERTEmbeddingModel</Method>
				<MLPythonLibrary>Tensorflow</MLPythonLibrary>
				<Pooling>REDUCE_MEAN</Pooling>
				<PoolingLayers>-2</PoolingLayers>
				<PretrainedModelDirectory>../BERTExperiments/BERTPretrainedModels/</PretrainedModelDirectory>
				<PretrainedModelName>biobert_v1.1_pubmed</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>Biosses2017StopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>BIOSSES</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/BERTPretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>biobert_v1.1_pubmed</PretrainedModelFilename>
					<NERType>None</NERType>
				</WordProcessing>
			</BertEmbeddingModelMeasure>

			<BertEmbeddingModelMeasure>
				<Label>biobert_large_v1.1_pubmed</Label>
				<Method>BERTEmbeddingModel</Method>
				<MLPythonLibrary>Tensorflow</MLPythonLibrary>
				<Pooling>REDUCE_MEAN</Pooling>
				<PoolingLayers>-2</PoolingLayers>
				<PretrainedModelDirectory>../BERTExperiments/BERTPretrainedModels/</PretrainedModelDirectory>
				<PretrainedModelName>biobert_large_v1.1_pubmed</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>Biosses2017StopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>BIOSSES</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/BERTPretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>biobert_large_v1.1_pubmed</PretrainedModelFilename>
					<NERType>None</NERType>
				</WordProcessing>
			</BertEmbeddingModelMeasure>
			
			
		</SentenceSimilarityMeasures>
	</SingleDatasetSentenceSimilarityValuesExperiment>
	<SingleDatasetSentenceSimilarityValuesExperiment>
		<OutputFilename>BioSentenceSimFinalRawOutputFiles/raw_similarity_MedSTSFull.csv</OutputFilename>
		<DatasetDirectory>../SentenceSimDatasets</DatasetDirectory>
		<DatasetFilename>MedStsFullNormalized.tsv</DatasetFilename>
		<SentenceSimilarityMeasures>
			
			
			<BertEmbeddingModelMeasure>
				<Label>oubiobert-base-uncased</Label>
				<Method>BERTEmbeddingModel</Method>
				<MLPythonLibrary>Pytorch</MLPythonLibrary>
				<PretrainedModelDirectory></PretrainedModelDirectory>
				<PretrainedModelName>seiya/oubiobert-base-uncased</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/PytorchExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/PytorchExperiments/venv/bin/python</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>Biosses2017StopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>BIOSSES</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/BERTPretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>oubiobert-base-uncased</PretrainedModelFilename>
					<NERType>None</NERType>
				</WordProcessing>
			</BertEmbeddingModelMeasure>

			<BertEmbeddingModelMeasure>
				<Label>PubMedBERT-base-uncased-abstract-fulltext</Label>
				<Method>BERTEmbeddingModel</Method>
				<MLPythonLibrary>Pytorch</MLPythonLibrary>
				<PretrainedModelDirectory></PretrainedModelDirectory>
				<PretrainedModelName>microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/PytorchExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/PytorchExperiments/venv/bin/python</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>Biosses2017StopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>BIOSSES</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/BERTPretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>PubMedBERT-base-uncased-abstract-fulltext</PretrainedModelFilename>
					<NERType>None</NERType>
				</WordProcessing>
			</BertEmbeddingModelMeasure>

			<BertEmbeddingModelMeasure>
				<Label>PubMedBERT-base-uncased-abstract</Label>
				<Method>BERTEmbeddingModel</Method>
				<MLPythonLibrary>Pytorch</MLPythonLibrary>
				<PretrainedModelDirectory></PretrainedModelDirectory>
				<PretrainedModelName>microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/PytorchExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/PytorchExperiments/venv/bin/python</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>Biosses2017StopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>BIOSSES</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/BERTPretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>PubMedBERT-base-uncased-abstract</PretrainedModelFilename>
					<NERType>None</NERType>
				</WordProcessing>
			</BertEmbeddingModelMeasure>

			<BertEmbeddingModelMeasure>
				<Label>scibert_scivocab_uncased</Label>
				<Method>BERTEmbeddingModel</Method>
				<MLPythonLibrary>Tensorflow</MLPythonLibrary>
				<Pooling>REDUCE_MEAN</Pooling>
				<PoolingLayers>-2</PoolingLayers>
				<PretrainedModelDirectory>../BERTExperiments/BERTPretrainedModels/</PretrainedModelDirectory>
				<PretrainedModelName>scibert_scivocab_uncased</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>Biosses2017StopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>BIOSSES</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/BERTPretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>scibert_scivocab_uncased</PretrainedModelFilename>
					<NERType>None</NERType>
				</WordProcessing>
			</BertEmbeddingModelMeasure>

			

			<BertEmbeddingModelMeasure>
				<Label>biobert_v1.1_pubmed</Label>
				<Method>BERTEmbeddingModel</Method>
				<MLPythonLibrary>Tensorflow</MLPythonLibrary>
				<Pooling>REDUCE_MEAN</Pooling>
				<PoolingLayers>-2</PoolingLayers>
				<PretrainedModelDirectory>../BERTExperiments/BERTPretrainedModels/</PretrainedModelDirectory>
				<PretrainedModelName>biobert_v1.1_pubmed</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>Biosses2017StopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>BIOSSES</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/BERTPretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>biobert_v1.1_pubmed</PretrainedModelFilename>
					<NERType>None</NERType>
				</WordProcessing>
			</BertEmbeddingModelMeasure>

			<BertEmbeddingModelMeasure>
				<Label>biobert_large_v1.1_pubmed</Label>
				<Method>BERTEmbeddingModel</Method>
				<MLPythonLibrary>Tensorflow</MLPythonLibrary>
				<Pooling>REDUCE_MEAN</Pooling>
				<PoolingLayers>-2</PoolingLayers>
				<PretrainedModelDirectory>../BERTExperiments/BERTPretrainedModels/</PretrainedModelDirectory>
				<PretrainedModelName>biobert_large_v1.1_pubmed</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>Biosses2017StopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>BIOSSES</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/BERTPretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>biobert_large_v1.1_pubmed</PretrainedModelFilename>
					<NERType>None</NERType>
				</WordProcessing>
			</BertEmbeddingModelMeasure>
			
		</SentenceSimilarityMeasures>
	</SingleDatasetSentenceSimilarityValuesExperiment>
	<SingleDatasetSentenceSimilarityValuesExperiment>
		<OutputFilename>BioSentenceSimFinalRawOutputFiles/raw_similarity_CTR.csv</OutputFilename>
		<DatasetDirectory>../SentenceSimDatasets</DatasetDirectory>
		<DatasetFilename>CTRNormalized_averagedScore.tsv</DatasetFilename>
		<SentenceSimilarityMeasures>
			
			<BertEmbeddingModelMeasure>
				<Label>oubiobert-base-uncased</Label>
				<Method>BERTEmbeddingModel</Method>
				<MLPythonLibrary>Pytorch</MLPythonLibrary>
				<PretrainedModelDirectory></PretrainedModelDirectory>
				<PretrainedModelName>seiya/oubiobert-base-uncased</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/PytorchExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/PytorchExperiments/venv/bin/python</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>Biosses2017StopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>BIOSSES</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/BERTPretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>oubiobert-base-uncased</PretrainedModelFilename>
					<NERType>None</NERType>
				</WordProcessing>
			</BertEmbeddingModelMeasure>

			<BertEmbeddingModelMeasure>
				<Label>PubMedBERT-base-uncased-abstract-fulltext</Label>
				<Method>BERTEmbeddingModel</Method>
				<MLPythonLibrary>Pytorch</MLPythonLibrary>
				<PretrainedModelDirectory></PretrainedModelDirectory>
				<PretrainedModelName>microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/PytorchExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/PytorchExperiments/venv/bin/python</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>Biosses2017StopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>BIOSSES</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/BERTPretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>PubMedBERT-base-uncased-abstract-fulltext</PretrainedModelFilename>
					<NERType>None</NERType>
				</WordProcessing>
			</BertEmbeddingModelMeasure>

			<BertEmbeddingModelMeasure>
				<Label>PubMedBERT-base-uncased-abstract</Label>
				<Method>BERTEmbeddingModel</Method>
				<MLPythonLibrary>Pytorch</MLPythonLibrary>
				<PretrainedModelDirectory></PretrainedModelDirectory>
				<PretrainedModelName>microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/PytorchExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/PytorchExperiments/venv/bin/python</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>Biosses2017StopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>BIOSSES</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/BERTPretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>PubMedBERT-base-uncased-abstract</PretrainedModelFilename>
					<NERType>None</NERType>
				</WordProcessing>
			</BertEmbeddingModelMeasure>

			<BertEmbeddingModelMeasure>
				<Label>scibert_scivocab_uncased</Label>
				<Method>BERTEmbeddingModel</Method>
				<MLPythonLibrary>Tensorflow</MLPythonLibrary>
				<Pooling>REDUCE_MEAN</Pooling>
				<PoolingLayers>-2</PoolingLayers>
				<PretrainedModelDirectory>../BERTExperiments/BERTPretrainedModels/</PretrainedModelDirectory>
				<PretrainedModelName>scibert_scivocab_uncased</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>Biosses2017StopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>BIOSSES</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/BERTPretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>scibert_scivocab_uncased</PretrainedModelFilename>
					<NERType>None</NERType>
				</WordProcessing>
			</BertEmbeddingModelMeasure>

			

			<BertEmbeddingModelMeasure>
				<Label>biobert_v1.1_pubmed</Label>
				<Method>BERTEmbeddingModel</Method>
				<MLPythonLibrary>Tensorflow</MLPythonLibrary>
				<Pooling>REDUCE_MEAN</Pooling>
				<PoolingLayers>-2</PoolingLayers>
				<PretrainedModelDirectory>../BERTExperiments/BERTPretrainedModels/</PretrainedModelDirectory>
				<PretrainedModelName>biobert_v1.1_pubmed</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>Biosses2017StopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>BIOSSES</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/BERTPretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>biobert_v1.1_pubmed</PretrainedModelFilename>
					<NERType>None</NERType>
				</WordProcessing>
			</BertEmbeddingModelMeasure>

			<BertEmbeddingModelMeasure>
				<Label>biobert_large_v1.1_pubmed</Label>
				<Method>BERTEmbeddingModel</Method>
				<MLPythonLibrary>Tensorflow</MLPythonLibrary>
				<Pooling>REDUCE_MEAN</Pooling>
				<PoolingLayers>-2</PoolingLayers>
				<PretrainedModelDirectory>../BERTExperiments/BERTPretrainedModels/</PretrainedModelDirectory>
				<PretrainedModelName>biobert_large_v1.1_pubmed</PretrainedModelName>
				<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
				<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python</PythonVirtualEnvironmentDir>
				<PythonScriptFilename>extractBERTvectors.py</PythonScriptFilename>
				<WordProcessing>
					<StopWordsFileDir>../StopWordsFiles</StopWordsFileDir>
					<StopWordsFilename>Biosses2017StopWords.txt</StopWordsFilename>
					<TokenizerType>WordPieceTokenizer</TokenizerType>
					<LowercaseNormalization>true</LowercaseNormalization>
					<CharFilteringType>BIOSSES</CharFilteringType>
					<PythonScriptsDirectory>../BERTExperiments/</PythonScriptsDirectory>
					<PythonVirtualEnvironmentDir>../BERTExperiments/venv/bin/python</PythonVirtualEnvironmentDir>
					<PythonWordPieceTokenizerScript>WordPieceTokenization.py</PythonWordPieceTokenizerScript>
					<PretrainedModelDirectory>../BERTExperiments/BERTPretrainedModels/</PretrainedModelDirectory>
					<PretrainedModelFilename>biobert_large_v1.1_pubmed</PretrainedModelFilename>
					<NERType>None</NERType>
				</WordProcessing>
			</BertEmbeddingModelMeasure>
			
		</SentenceSimilarityMeasures>
	</SingleDatasetSentenceSimilarityValuesExperiment>
</SentenceSimilarityExperiments>