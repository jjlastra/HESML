# Description:
#
# This script loads a collection of word similarity benchmarks generated
# by HESML, ehich contain the raw similarity values for each word pair.
# Then, the script computes a collection of consolidated tables
# including the Pearson and Spearman correlation metrics.
#
# References:
# ----------

# We clear all session variables

rm(list = ls())

# IMPORTANT:configuration of the input/output directories
# We define below the input directory for the input raw results
# in CSV file format, and the output directory for the
# final assembled tables in CSV file format.
# You must change these values in order to
# point to the proper directories in your hard drive.
# We also define below the name of the input raw CSV files
# containing the experimental results.

# The input and output directories below must end with '/' in
# Unix-like format to be compatible with Windows
# or Linux-based R distributions.

rootDir = "C:/HESML_GitHub/HESML_Library/ReproducibleExperiments/Embeddings_vs_OntologyMeasures_paper"
inputDir = paste(rootDir, sep = "/", "RawOutputFiles/")
outputDir =  paste(rootDir, sep = "/", "ProcessedOutputFiles/")

dir.create(outputDir)

# Input raw CSV files generated by the reproducible experiments detailed
# in the companion paper.

raw_MC28_file <- "raw_similarity_values_MC28_dataset.csv"
raw_RG65_file <- "raw_similarity_values_RG65_dataset.csv"
raw_PSfull_file <- "raw_similarity_values_PSfull_dataset.csv"
raw_Agirre201_file <- "raw_similarity_values_Agirre201_lowercase_dataset.csv"
raw_SimLex665_file <- "raw_similarity_values_SimLex665_dataset.csv"
raw_MTurk771_file <- "raw_similarity_values_MTurk771_dataset.csv"
raw_MTurk287_235_file  <- "raw_similarity_values_MTurk287-235_dataset.csv"
raw_WS353Rel_file <- "raw_similarity_values_WS353Rel_dataset.csv"
raw_Rel122_file <- "raw_similarity_values_Rel122_dataset.csv"
raw_WS353Full_file <- "raw_similarity_values_WS353Full_dataset.csv"
raw_SimLex111_file <- "raw_similarity_values_SimLex111_dataset.csv"
raw_SimLex222_file <- "raw_similarity_values_SimLex222_dataset.csv"
raw_SimLex999_file <- "raw_similarity_values_SimLex999_dataset.csv"
raw_SimVerb3500_file <- "raw_similarity_values_SimVerb3500_dataset.csv"
raw_MEN_file <- "raw_similarity_values_MEN_dataset.csv"
raw_YP130_file <- "raw_similarity_values_YP130_dataset.csv"
raw_RareWords2034_file <- "raw_similarity_values_RareWords2034_dataset.csv"
raw_RareWords1401_file <- "raw_similarity_values_RareWords1401_dataset.csv"
raw_SCWS1994_file <- "raw_similarity_values_SCWS1994_dataset.csv"

# We load the input raw results file

rawdata_MC28 <- read.csv(paste(inputDir, sep = "", raw_MC28_file),dec = ".", sep = ';')
rawdata_RG65 <- read.csv(paste(inputDir, sep = "", raw_RG65_file),dec = ".", sep = ';')
rawdata_PSfull <- read.csv(paste(inputDir, sep = "", raw_PSfull_file),dec = ".", sep = ';')
rawdata_Agirre201 <- read.csv(paste(inputDir, sep = "", raw_Agirre201_file),dec = ".", sep = ';')
rawdata_SimLex665 <- read.csv(paste(inputDir, sep = "", raw_SimLex665_file),dec = ".", sep = ';')
rawdata_MTurk771 <- read.csv(paste(inputDir, sep = "", raw_MTurk771_file),dec = ".", sep = ';')
rawdata_MTurk287_235 <- read.csv(paste(inputDir, sep = "", raw_MTurk287_235_file),dec = ".", sep = ';')
rawdata_WS353Rel <- read.csv(paste(inputDir, sep = "", raw_WS353Rel_file),dec = ".", sep = ';')
rawdata_Rel122 <- read.csv(paste(inputDir, sep = "", raw_Rel122_file),dec = ".", sep = ';')
rawdata_WS353Full <- read.csv(paste(inputDir, sep = "", raw_WS353Full_file),dec = ".", sep = ';')
rawdata_SimLex111 <- read.csv(paste(inputDir, sep = "", raw_SimLex111_file),dec = ".", sep = ';')
rawdata_SimLex222 <- read.csv(paste(inputDir, sep = "", raw_SimLex222_file),dec = ".", sep = ';')
rawdata_SimLex999 <- read.csv(paste(inputDir, sep = "", raw_SimLex999_file),dec = ".", sep = ';')
rawdata_SimVerb3500 <- read.csv(paste(inputDir, sep = "", raw_SimVerb3500_file),dec = ".", sep = ';')
rawdata_MEN <- read.csv(paste(inputDir, sep = "", raw_MEN_file),dec = ".", sep = ';')
rawdata_YP130 <- read.csv(paste(inputDir, sep = "", raw_YP130_file),dec = ".", sep = ';')
rawdata_RareWords2034 <- read.csv(paste(inputDir, sep = "", raw_RareWords2034_file),dec = ".", sep = ';')
rawdata_RareWords1401 <- read.csv(paste(inputDir, sep = "", raw_RareWords1401_file),dec = ".", sep = ';')
rawdata_SCWS1994 <- read.csv(paste(inputDir, sep = "", raw_SCWS1994_file),dec = ".", sep = ';')

# mat.sort function is copied from source files of
# BioPhysConnectoR package which is now unavailable.
# Source code was retrieved from https://rdrr.io/cran/BioPhysConnectoR/src/R/mat.sort.r

mat.sort<-function(mat,sort,decreasing=FALSE){
  m<-do.call("order",c(as.data.frame(mat[,sort]),decreasing=decreasing))
  mat[m,]
} 

# ---------------------------------------------------------------------
# Raw output file format:
# Raw similarity files contain the similarity values returned by each
# semantic measure for each word pair. First nine dataset were evaluated
# with a set of 21 ontology-based measures and 11 word embedding models,
# whilst remaining datasets were only evaluated with the word embeddings.
# First two columns contain the word pairs and the human judgements,
# whilst subsequent columns contain the values returned by the
# twenty-one ontology-based measures and the eleven word embedding models.
# ---------------------------------------------------------------------

# ---------------------------------------------------------------------
# Tables 1,2 and 3: Pearson and Spearman metrics of all measures
# and embeddings in the 5 similarity datasets evaluated both by the
# ontology-based measures based on WordNet as the pre-trained word
# embedding models.
# ---------------------------------------------------------------------

# We define all datasets represented in table 1

rawdataSimNounDatasets <- list(rawdata_MC28, rawdata_RG65, rawdata_PSfull, rawdata_Agirre201, rawdata_SimLex665)

# We create a separated table for each metric by removing first two columns
# which contain the word pairs and human judgements. 
# Measures are aranged in rows whilst datasets are arranged in columns.

table_Pearson_SimDatasets <- matrix(nrow = ncol(rawdata_MC28) - 2,
                                  ncol = length(rawdataSimNounDatasets),
                                  dimnames = list(colnames(rawdata_MC28)[3:ncol(rawdata_MC28)],
                                                  c("MC28", "RG65", "PSfull", "Agirre201", "SimLex665")))

table_Spearman_SimDatasets <- table_Pearson_SimDatasets

# Loop for the computation of the metrics

nMeasures <- nrow(table_Pearson_SimDatasets)
nDatasets <- length(rawdataSimNounDatasets)

for (iDataset in 1:nDatasets)
{
	# We get the raw data of the next dataset

	rawdata <- rawdataSimNounDatasets[[iDataset]]

	# We evaluate the Pearson and Spearman metrics for each measure in the current dataset

	for (iMeasure in 1:nMeasures)
	{
		table_Pearson_SimDatasets[iMeasure, iDataset] <- cor(rawdata[,2], rawdata[, iMeasure + 2], method = "pearson")
		table_Spearman_SimDatasets[iMeasure, iDataset] <- cor(rawdata[,2], rawdata[, iMeasure + 2], method = "spearman")
	}
}

# We compute the p-values testing the hypothesis that the best performing measure, Attract-reppel model,
# outperforms significantly the rest of methods in noun similarity datsets

pvalues_SimDatasets_Pearson <- matrix(nrow = nrow(table_Pearson_SimDatasets), ncol = 1)
colnames(pvalues_SimDatasets_Pearson) = "p-value"

pvalues_SimDatasets_Spearman <- matrix(nrow = nrow(table_Spearman_SimDatasets), ncol = 1)
colnames(pvalues_SimDatasets_Spearman) = "p-value"

iAttractReppel <- 22

for (iMeasure in 1:nMeasures)
{
  pvalues_SimDatasets_Pearson[iMeasure, 1] <- signif(t.test(table_Pearson_SimDatasets[iAttractReppel, ],
                                                            table_Pearson_SimDatasets[iMeasure, ],
                                                               paired = TRUE,alternative="greater")$p.value,
                                                        digits=2)
  
  pvalues_SimDatasets_Spearman[iMeasure, 1] <- signif(t.test(table_Spearman_SimDatasets[iAttractReppel, ],
                                                             table_Spearman_SimDatasets[iMeasure, ],
                                                                paired = TRUE,alternative="greater")$p.value,
                                                         digits=2)
}

# We save a copy of Pearson and Spearman tables before to add further information
# with the aim of using them in the computation of p-values for the averages measures below

backup_table_Pearson_SimDatasets <- table_Pearson_SimDatasets
backup_table_Spearman_SimDatasets <- table_Spearman_SimDatasets

# We compute the average values per row and sort the rows

table_Pearson_SimDatasets <- cbind(table_Pearson_SimDatasets,
                                   Avg = rowMeans(table_Pearson_SimDatasets[1:nrow(table_Pearson_SimDatasets),]),
                                   pvalues_SimDatasets_Pearson)

table_Pearson_SimDatasets <- mat.sort(table_Pearson_SimDatasets, ncol(table_Pearson_SimDatasets) - 1, decreasing = TRUE)

table_Spearman_SimDatasets <- cbind(table_Spearman_SimDatasets,
                                    Avg = rowMeans(table_Spearman_SimDatasets[1:nrow(table_Spearman_SimDatasets),]),
                                    pvalues_SimDatasets_Spearman)

table_Spearman_SimDatasets <- mat.sort(table_Spearman_SimDatasets, ncol(table_Spearman_SimDatasets) - 1, decreasing = TRUE)

# We make a copy of the tables in order to round their values to 3 decimal digits

table_Pearson_SimDatasets_rounded <- round(table_Pearson_SimDatasets, 3);
table_Spearman_SimDatasets_rounded <- round(table_Spearman_SimDatasets, 3);

# We save all final assembled data tables 

write.csv(table_Pearson_SimDatasets, file = paste(outputDir, sep="","table_Pearson_SimDatasets.csv"))
write.csv(table_Pearson_SimDatasets_rounded, file = paste(outputDir, sep="","table_Pearson_SimDatasets_rounded.csv"))

write.csv(table_Spearman_SimDatasets, file = paste(outputDir, sep="","table_Spearman_SimDatasets.csv"))
write.csv(table_Spearman_SimDatasets_rounded, file = paste(outputDir, sep="","table_Spearman_SimDatasets_rounded.csv"))

# ---------------------------------------------------------------------
# Table 4,5 and 6: Pearson, Spearman and Harmonic mean metrics of all measures
# and embeddings in the 4 relatedness datasets evaluated both by the
# ontoloy-based measures basedon Wordnet as the pre-trained word
# embedding models.
# ---------------------------------------------------------------------

# We define all relatedness datasets represented in tables

rawdataRelNounDatasets <- list(rawdata_MTurk771, rawdata_MTurk287_235, rawdata_WS353Rel, rawdata_Rel122)

# We create the table 2

# We create a separated table for each metric

table_Pearson_RelDatasets <- matrix(nrow = ncol(rawdata_MC28) - 2,
                                    ncol = length(rawdataRelNounDatasets),
                                    dimnames = list(colnames(rawdata_MC28)[3:ncol(rawdata_MC28)],
                                                    c("MTurk771", "MTurk287_235", "WS353Rel", "Rel122")))

table_Spearman_RelDatasets <- table_Pearson_RelDatasets

# Loop for the computation of all metrics

nMeasures <- nrow(table_Pearson_RelDatasets)
nDatasets <- length(rawdataRelNounDatasets)

for (iDataset in 1:nDatasets)
{
	# We get the raw data of the next dataset

	rawdata <- rawdataRelNounDatasets[[iDataset]]

	# We evaluate the Pearson, Spearman and Harmonic metrics for each measure in the current dataset

	for (iMeasure in 1:nMeasures)
	{
		table_Pearson_RelDatasets[iMeasure, iDataset] <- cor(rawdata[,2], rawdata[, iMeasure + 2], method = "pearson")
		table_Spearman_RelDatasets[iMeasure, iDataset] <- cor(rawdata[,2], rawdata[, iMeasure + 2], method = "spearman")
	}
}

# We compute the p-values testing the hypothesis that the best performing measure, Paragram-ws model,
# outperforms significantly the rest of methods in noun relatedness datsets

pvalues_RelDatasets_Pearson <- matrix(nrow = nrow(table_Pearson_RelDatasets), ncol = 1)
colnames(pvalues_RelDatasets_Pearson) = "p-value"

pvalues_RelDatasets_Spearman <- matrix(nrow = nrow(table_Spearman_RelDatasets), ncol = 1)
colnames(pvalues_RelDatasets_Spearman) = "p-value"

iParagram_ws <- 27

for (iMeasure in 1:nMeasures)
{
  pvalues_RelDatasets_Pearson[iMeasure, 1] <- signif(t.test(table_Pearson_RelDatasets[iParagram_ws, ],
                                                            table_Pearson_RelDatasets[iMeasure, ],
                                                            paired = TRUE,alternative="greater")$p.value,
                                                     digits=2)
  
  pvalues_RelDatasets_Spearman[iMeasure, 1] <- signif(t.test(table_Spearman_RelDatasets[iParagram_ws, ],
                                                             table_Spearman_RelDatasets[iMeasure, ],
                                                             paired = TRUE,alternative="greater")$p.value,
                                                      digits=2)
}

# We save a copy of Pearson and Spearman tables before to add further information
# with the aim of using them in the computtion of p-values for the averages measures below

backup_table_Pearson_RelDatasets <- table_Pearson_RelDatasets
backup_table_Spearman_RelDatasets <- table_Spearman_RelDatasets

# We compute the average values per row and sort the rows

table_Pearson_RelDatasets <- cbind(table_Pearson_RelDatasets,
                                   Avg = rowMeans(table_Pearson_RelDatasets[1:nrow(table_Pearson_RelDatasets),]),
                                   pvalues_RelDatasets_Pearson)

table_Pearson_RelDatasets <- mat.sort(table_Pearson_RelDatasets, ncol(table_Pearson_RelDatasets) - 1, decreasing = TRUE)

table_Spearman_RelDatasets <- cbind(table_Spearman_RelDatasets,
                                    Avg = rowMeans(table_Spearman_RelDatasets[1:nrow(table_Spearman_RelDatasets),]),
                                    pvalues_RelDatasets_Spearman)

table_Spearman_RelDatasets <- mat.sort(table_Spearman_RelDatasets, ncol(table_Spearman_RelDatasets) - 1, decreasing = TRUE)

# We make a copy of the tables in order to round their values to 3 decimal digits

table_Pearson_RelDatasets_rounded <- round(table_Pearson_RelDatasets, 3);
table_Spearman_RelDatasets_rounded <- round(table_Spearman_RelDatasets, 3);

# We save all final assembled data tables 

write.csv(table_Pearson_RelDatasets, file = paste(outputDir, sep="","table_Pearson_RelDatasets.csv"))
write.csv(table_Pearson_RelDatasets_rounded, file = paste(outputDir, sep="","table_Pearson_RelDatasets_rounded.csv"))

write.csv(table_Spearman_RelDatasets, file = paste(outputDir, sep="","table_Spearman_RelDatasets.csv"))
write.csv(table_Spearman_RelDatasets_rounded, file = paste(outputDir, sep="","table_Spearman_RelDatasets_rounded.csv"))

# ---------------------------------------------------------------------
# Table 7, 8 and 9: Pearson, Spearman and Harmonic metrics of all
# pre-trained embeddings models in all datasets. Rows contain datasets
# whilst columns contain the perfromance of each word embedding model
# in each dataset. Columns are sorted in descending order from
# left to right. Leftmost columns show best performing embedding models.
# ---------------------------------------------------------------------

# We define all datasets

rawdataAllDatasets<-list(rawdata_MC28, rawdata_RG65, rawdata_PSfull, rawdata_Agirre201, rawdata_SimLex665, rawdata_SimLex111, rawdata_SimLex222, rawdata_SimLex999, rawdata_SimVerb3500, rawdata_MTurk771, rawdata_MTurk287_235, rawdata_WS353Rel, rawdata_Rel122, rawdata_WS353Full, rawdata_MEN, rawdata_YP130, rawdata_RareWords2034, rawdata_RareWords1401, rawdata_SCWS1994);
rawdataAllDatasetNames<-c("MC28", "RG65", "PSfull", "Agirre201", "SimLex665", "SimLex111", "SimLex222", "SimLex999", "SimVerb3500", "MTurk771", "MTurk287_235", "WS353Rel", "Rel122", "WS353Full", "MEN", "YP130", "RW2034", "RW1401", "SCWS1994")

# We create the tables 3,4 and 5

table_Pearson_allEmbeddings <- matrix(ncol = 11, nrow = length(rawdataAllDatasets),
                                      dimnames = list(rawdataAllDatasetNames,
                                                      colnames(rawdata_WS353Full)[3:ncol(rawdata_WS353Full)]))

table_Spearman_allEmbeddings <- table_Pearson_allEmbeddings
table_Harmonic_allEmbeddings <- table_Pearson_allEmbeddings

nOntologyBasedMeasures = 21;
nEmbeddings <- ncol(table_Pearson_allEmbeddings)
nDatasets <- length(rawdataAllDatasets)

for (iDataset in 1:nDatasets)
{
	# We get the raw data of the next dataset

	rawdata <- rawdataAllDatasets[[iDataset]]

	# We define the offset position to extract the column of each embedding model.
	# First nine raw input datasets include the evaluation of 21 ontology-based measures and
	# 11 embedding models. However, the remaining 10 datasets are only evaluated
	# on the embedding models.

	if ((iDataset <= 5) || ((iDataset >= 10) && (iDataset <= 13)))
	{
		iOffset <- nOntologyBasedMeasures + 2;
	}
	else
	{
		iOffset <- 2;
	}

	# We evaluate the Pearson, Spearman and Harmonic metrics for each
	# embeding model in the current dataset

	for (iEmbedding in 1:nEmbeddings)
	{
		table_Pearson_allEmbeddings[iDataset, iEmbedding] <- cor(rawdata[,2], rawdata[, iEmbedding + iOffset], method = "pearson")
		table_Spearman_allEmbeddings[iDataset, iEmbedding] <- cor(rawdata[,2], rawdata[, iEmbedding + iOffset], method = "spearman")
		table_Harmonic_allEmbeddings[iDataset, iEmbedding] <- 2 * table_Pearson_allEmbeddings[iDataset, iEmbedding] * table_Spearman_allEmbeddings[iDataset, iEmbedding] / (table_Pearson_allEmbeddings[iDataset, iEmbedding] + table_Spearman_allEmbeddings[iDataset, iEmbedding])
	}
}

#-----------------------------------------------------------------
# PERFORMANCE of word embeddings only in SIMILARITY DATASETS
# We extract the performance of all models in all similarity datasets
#-----------------------------------------------------------------

# We extract the similarity datasets

table_Pearson_allEmbeddings_similarity <- table_Pearson_allEmbeddings[1:9,]
table_Pearson_allEmbeddings_similarity <- rbind(table_Pearson_allEmbeddings_similarity, Avg_r = colMeans(table_Pearson_allEmbeddings_similarity[,1:ncol(table_Pearson_allEmbeddings_similarity)]))

table_Spearman_allEmbeddings_similarity <- table_Spearman_allEmbeddings[1:9,]
table_Spearman_allEmbeddings_similarity <- rbind(table_Spearman_allEmbeddings_similarity, Avg_rho = colMeans(table_Spearman_allEmbeddings_similarity[,1:ncol(table_Spearman_allEmbeddings_similarity)]))

table_Harmonic_allEmbeddings_similarity <- table_Harmonic_allEmbeddings[1:9,]
table_Harmonic_allEmbeddings_similarity <- rbind(table_Harmonic_allEmbeddings_similarity, Avg_h = colMeans(table_Harmonic_allEmbeddings_similarity[,1:ncol(table_Harmonic_allEmbeddings_similarity)]))

table_joined_allEmbeddings_similarity = rbind(table_Pearson_allEmbeddings_similarity, table_Spearman_allEmbeddings_similarity, table_Harmonic_allEmbeddings_similarity)

# We transpose the matrices in order to sort them according to the
# average value obtained by each embedding model

table_joined_allEmbeddings_similarity <- t(table_joined_allEmbeddings_similarity)
table_joined_allEmbeddings_similarity <- mat.sort(table_joined_allEmbeddings_similarity, ncol(table_joined_allEmbeddings_similarity), decreasing = TRUE)
table_joined_allEmbeddings_similarity <- t(table_joined_allEmbeddings_similarity)

#-----------------------------------------------------------------
# PERFORMANCE of word embeddings only in RELATEDNESS DATASETS
# We extract the performance of all models in all similarity datasets
#-----------------------------------------------------------------

table_Pearson_allEmbeddings_relatedness <- table_Pearson_allEmbeddings[10:nrow(table_Pearson_allEmbeddings),]
table_Pearson_allEmbeddings_relatedness <- rbind(table_Pearson_allEmbeddings_relatedness, Avg_r = colMeans(table_Pearson_allEmbeddings_relatedness[,1:ncol(table_Pearson_allEmbeddings_relatedness)]))

table_Spearman_allEmbeddings_relatedness <- table_Spearman_allEmbeddings[10:nrow(table_Spearman_allEmbeddings),]
table_Spearman_allEmbeddings_relatedness <- rbind(table_Spearman_allEmbeddings_relatedness, Avg_rho = colMeans(table_Spearman_allEmbeddings_relatedness[,1:ncol(table_Spearman_allEmbeddings_relatedness)]))

table_Harmonic_allEmbeddings_relatedness <- table_Harmonic_allEmbeddings[10:nrow(table_Harmonic_allEmbeddings),]
table_Harmonic_allEmbeddings_relatedness <- rbind(table_Harmonic_allEmbeddings_relatedness, Avg_h = colMeans(table_Harmonic_allEmbeddings_relatedness[,1:ncol(table_Harmonic_allEmbeddings_relatedness)]))

table_joined_allEmbeddings_relatedness <- rbind(table_Pearson_allEmbeddings_relatedness, table_Spearman_allEmbeddings_relatedness, table_Harmonic_allEmbeddings_relatedness)

# We transpose the matrices in order to sort them according to the
# average value obtained by each embedding model

table_joined_allEmbeddings_relatedness <- t(table_joined_allEmbeddings_relatedness)
table_joined_allEmbeddings_relatedness <- mat.sort(table_joined_allEmbeddings_relatedness, ncol(table_joined_allEmbeddings_relatedness), decreasing = TRUE)
table_joined_allEmbeddings_relatedness <- t(table_joined_allEmbeddings_relatedness)

# We make a copy of the tables in order to round their values to 3 decimal digits

table_joined_allEmbeddings_similarity_rounded <- round(table_joined_allEmbeddings_similarity, 3)
table_joined_allEmbeddings_relatedness_rounded <- round(table_joined_allEmbeddings_relatedness, 3)

# We save all the final assembled data tables 

write.csv(table_joined_allEmbeddings_similarity, file = paste(outputDir, sep="","table_joined_allEmbeddings_similarity.csv"))
write.csv(table_joined_allEmbeddings_similarity_rounded, file = paste(outputDir, sep="","table_joined_allEmbeddings_similarity_rounded.csv"))

write.csv(table_joined_allEmbeddings_relatedness, file = paste(outputDir, sep="","table_joined_allEmbeddings_relatedness.csv"))
write.csv(table_joined_allEmbeddings_relatedness_rounded, file = paste(outputDir, sep="","table_joined_allEmbeddings_relatedness_rounded.csv"))

# ---------------------------------------------------------------------
# Table of p-values between Attract-reppel model and the remaining
# word embedding models in following tables:
#
# (1) table_Pearson_allEmbeddings_similarity
# (2) table_Spearman_allEmbeddings_similarity
# (3) table_Harmonic_allEmbeddings_similarity
#
# This information is used to draw strong conclusions on the performance
# of Attract-reppel model.
# ---------------------------------------------------------------------

# We initialize the p-value vectors incuding their names

pvalues_r <- matrix(ncol = 1, nrow = ncol(table_Pearson_allEmbeddings_similarity) - 1)
pvalues_rho <- matrix(ncol = 1, nrow = ncol(table_Spearman_allEmbeddings_similarity) - 1)
pvalues_harmonic <- matrix(ncol = 1, nrow = ncol(table_Harmonic_allEmbeddings_similarity) - 1)

rownames(pvalues_r) <- c(1:nrow(pvalues_r))
rownames(pvalues_rho) <- c(1:nrow(pvalues_rho))
rownames(pvalues_harmonic) <- c(1:nrow(pvalues_harmonic))

for (iMeasure in 1:nrow(pvalues_r))
{
  # We set the names of the measures evaluated
  
  rownames(pvalues_r)[iMeasure] <- colnames(table_Pearson_allEmbeddings_similarity)[iMeasure + 1]
  rownames(pvalues_rho)[iMeasure] <- colnames(table_Spearman_allEmbeddings_similarity)[iMeasure + 1]
  rownames(pvalues_harmonic)[iMeasure] <- colnames(table_Harmonic_allEmbeddings_similarity)[iMeasure + 1]
  
  # We set the p-values
  
  pvalues_r[iMeasure] <- signif(t.test(table_Pearson_allEmbeddings_similarity[1:9, 1],
                                       table_Pearson_allEmbeddings_similarity[1:9, iMeasure + 1],
                                       paired = TRUE,alternative="greater")$p.value,
                                digits=2)
  
  
  pvalues_rho[iMeasure] <- signif(t.test(table_Spearman_allEmbeddings_similarity[1:9, 1],
                                         table_Spearman_allEmbeddings_similarity[1:9, iMeasure + 1],
                                         paired = TRUE,alternative="greater")$p.value,
                                  digits=2)
  
  
  pvalues_harmonic[iMeasure] <- signif(t.test(table_Harmonic_allEmbeddings_similarity[1:9, 1],
                                              table_Harmonic_allEmbeddings_similarity[1:9, iMeasure + 1],
                                              paired = TRUE,alternative="greater")$p.value,
                                       digits=2)
}

# We sort the p-values in descending order

pvalues_r <- mat.sort(pvalues_r, 1, decreasing = TRUE)
pvalues_rho <- mat.sort(pvalues_rho, ncol(pvalues_rho), decreasing = TRUE)
pvalues_harmonic <- mat.sort(pvalues_harmonic, ncol(pvalues_harmonic), decreasing = TRUE)

# We group p-values in a matrix

table_pvalues_AttractReppel_allembeddings_similarity <- matrix(ncol = 6, nrow = ncol(table_Pearson_allEmbeddings_similarity) - 1)

colnames(table_pvalues_AttractReppel_allembeddings_similarity)<-c("Measure","p-value(r)","Measure","p-value(rho)","Measure","p-value(h)")

table_pvalues_AttractReppel_allembeddings_similarity[,1] <- names(pvalues_r)
table_pvalues_AttractReppel_allembeddings_similarity[,2] <- pvalues_r
table_pvalues_AttractReppel_allembeddings_similarity[,3] <- names(pvalues_rho)
table_pvalues_AttractReppel_allembeddings_similarity[,4] <- pvalues_rho
table_pvalues_AttractReppel_allembeddings_similarity[,5] <- names(pvalues_harmonic)
table_pvalues_AttractReppel_allembeddings_similarity[,6] <- pvalues_harmonic

write.csv(table_pvalues_AttractReppel_allembeddings_similarity, file = paste(outputDir, sep="","table_pvalues_AttractReppel_allembeddings_similarity.csv"))

# ---------------------------------------------------------------------
# Table of p-values between Paragra,-ws model and the remaining
# word embedding models in following tables:
#
# (1) table_Pearson_allEmbeddings_relatedness
# (2) table_Spearman_allEmbeddings__relatedness
# (3) table_Harmonic_allEmbeddings__relatedness
#
# This information is used to draw strong conclusions on the performance
# of Attract-reppel model.
# ---------------------------------------------------------------------

# We initialize the p-value vectors incuding their names

pvalues_r <- matrix(ncol = 1, nrow = ncol(table_Pearson_allEmbeddings_relatedness) - 1)
pvalues_rho <- matrix(ncol = 1, nrow = ncol(table_Spearman_allEmbeddings_relatedness) - 1)
pvalues_harmonic <- matrix(ncol = 1, nrow = ncol(table_Harmonic_allEmbeddings_relatedness) - 1)

rownames(pvalues_r) <- c(1:nrow(pvalues_r))
rownames(pvalues_rho) <- c(1:nrow(pvalues_rho))
rownames(pvalues_harmonic) <- c(1:nrow(pvalues_harmonic))

indexMeasures <-c(1:5,7:11)

for (iMeasure in 1:nrow(pvalues_r))
{
  # We set the names of the measures evaluated
  
  rownames(pvalues_r)[iMeasure] <- colnames(table_Pearson_allEmbeddings_relatedness)[indexMeasures[iMeasure]]
  rownames(pvalues_rho)[iMeasure] <- colnames(table_Spearman_allEmbeddings_relatedness)[indexMeasures[iMeasure]]
  rownames(pvalues_harmonic)[iMeasure] <- colnames(table_Harmonic_allEmbeddings_relatedness)[indexMeasures[iMeasure]]
  
  # We set the p-values. Paragram-ws is in the sixth column.
  
  pvalues_r[iMeasure] <- signif(t.test(table_Pearson_allEmbeddings_relatedness[1:10, 6],
                                       table_Pearson_allEmbeddings_relatedness[1:10, indexMeasures[iMeasure]],
                                       paired = TRUE,alternative="greater")$p.value,
                                digits=2)
  
  
  pvalues_rho[iMeasure] <- signif(t.test(table_Spearman_allEmbeddings_relatedness[1:10, 6],
                                         table_Spearman_allEmbeddings_relatedness[1:10, indexMeasures[iMeasure]],
                                         paired = TRUE,alternative="greater")$p.value,
                                  digits=2)
  
  
  pvalues_harmonic[iMeasure] <- signif(t.test(table_Harmonic_allEmbeddings_relatedness[1:10, 6],
                                              table_Harmonic_allEmbeddings_relatedness[1:10, indexMeasures[iMeasure]],
                                              paired = TRUE,alternative="greater")$p.value,
                                       digits=2)
}

# We sort the p-values in descending order

pvalues_r <- mat.sort(pvalues_r, 1, decreasing = TRUE)
pvalues_rho <- mat.sort(pvalues_rho, ncol(pvalues_rho), decreasing = TRUE)
pvalues_harmonic <- mat.sort(pvalues_harmonic, ncol(pvalues_harmonic), decreasing = TRUE)

# We group p-values in a matrix

table_pvalues_Paragramws_allembeddings_relatedness <- matrix(ncol = 6, nrow = ncol(table_Pearson_allEmbeddings_relatedness) - 1)

colnames(table_pvalues_Paragramws_allembeddings_relatedness)<-c("Measure","p-value(r)","Measure","p-value(rho)","Measure","p-value(h)")

table_pvalues_Paragramws_allembeddings_relatedness[,1] <- names(pvalues_r)
table_pvalues_Paragramws_allembeddings_relatedness[,2] <- pvalues_r
table_pvalues_Paragramws_allembeddings_relatedness[,3] <- names(pvalues_rho)
table_pvalues_Paragramws_allembeddings_relatedness[,4] <- pvalues_rho
table_pvalues_Paragramws_allembeddings_relatedness[,5] <- names(pvalues_harmonic)
table_pvalues_Paragramws_allembeddings_relatedness[,6] <- pvalues_harmonic

write.csv(table_pvalues_Paragramws_allembeddings_relatedness, file = paste(outputDir, sep="","table_pvalues_Paragramws_allembeddings_relatedness.csv"))

# ---------------------------------------------------------------------
# COMPUTATION OF AVERAGED MODELS OF SIMILARITY
#
# This section computes the semantic similarity resulting from
# the averaging of the best similarity measure in the noun similarity
# datasets with each other measure, both WE and OM measures.
# ---------------------------------------------------------------------

# EVALUATION OF NOUN SIMILARITY DATASETS
#
#  We create two tables for Pearson and Spearman metrics
# by removing first two columns which contain the word pairs and human judgements. 
# Measures are arranged in rows whilst datasets are arranged in columns.
#  These two tables reproduce the same tables reporting the Pearson and
# Spearman correlation values of all measure in the five noun similarity
# datasets but using the averaged measures.
#  Input data is defined by the vectors of raw similarity values returned
# by each measure.

table_AvgMeasures_Spearman_SimDatasets <- matrix(nrow = ncol(rawdata_MC28) - 2,
                                    ncol = length(rawdataSimNounDatasets),
                                    dimnames = list(colnames(rawdata_MC28)[3:ncol(rawdata_MC28)],
                                                    c("MC28", "RG65", "PSfull", "Agirre201", "SimLex665")))

table_AvgMeasures_Pearson_SimDatasets <- table_AvgMeasures_Spearman_SimDatasets

# Loop for the computation of the metrics

nMeasures <- nrow(table_AvgMeasures_Spearman_SimDatasets)
nDatasets <- length(rawdataSimNounDatasets)

# We extract the best perfomring similarity measure (Attract-reppel)

iBestSimMeasure <- 22

for (iDataset in 1:nDatasets)
{
  # We get the raw data of the next dataset
  
  rawdata <- rawdataSimNounDatasets[[iDataset]]

  # We get the human judgement vector and the raw similarity values
  # of the best performing measure in the current dataset
  
  humanJugments <- rawdata[,2]
  rawbestMeasure <- rawdata[, iBestSimMeasure + 2]
  
  # We evaluate the Spearman correlation for each averaged measure in the current dataset
  
  for (iMeasure in 1:nMeasures)
  {
    # We compute the average similarity values corresponding to
    # the combination of the best performing measure with iMeasure
    
    currentMeasure <- rawdata[, iMeasure + 2]
    averaged_sim <- 0.5 * (currentMeasure + rawbestMeasure)      
    
    # We compute the Spearman correlation of the averaged measure
    
    table_AvgMeasures_Spearman_SimDatasets[iMeasure, iDataset] <- cor(humanJugments, averaged_sim, method = "spearman")
    table_AvgMeasures_Pearson_SimDatasets[iMeasure, iDataset] <- cor(humanJugments, averaged_sim, method = "pearson")
  }
}

# We compute the p-values comparing the performance of averaged measures with
# their base counterpart. The aim is to test the hypothesis that the combination
# of each measure with the best performing one outperforms the individual measure

avgSimDatasets_Pearson_pvalues <- matrix(nrow = nrow(table_AvgMeasures_Pearson_SimDatasets), ncol = 1)
colnames(avgSimDatasets_Pearson_pvalues) = "p-value"

avgSimDatasets_Spearman_pvalues <- matrix(nrow = nrow(table_AvgMeasures_Spearman_SimDatasets), ncol = 1)
colnames(avgSimDatasets_Spearman_pvalues) = "p-value"

for (iMeasure in 1:nMeasures)
{
  avgSimDatasets_Pearson_pvalues[iMeasure, 1] <- signif(t.test(table_AvgMeasures_Pearson_SimDatasets[iMeasure, ],
                                                backup_table_Pearson_SimDatasets[iMeasure, ],
                                                paired = TRUE,alternative="greater")$p.value,
                                                digits=2)
  
  avgSimDatasets_Spearman_pvalues[iMeasure, 1] <- signif(t.test(table_AvgMeasures_Spearman_SimDatasets[iMeasure, ],
                                                               backup_table_Spearman_SimDatasets[iMeasure, ],
                                                               paired = TRUE,alternative="greater")$p.value,
                                                        digits=2)
}

# We compute the average values per row and sort the rows

table_AvgMeasures_Spearman_SimDatasets <- cbind(table_AvgMeasures_Spearman_SimDatasets,
                                                Avg = rowMeans(table_AvgMeasures_Spearman_SimDatasets[1:nrow(table_AvgMeasures_Spearman_SimDatasets),]),
                                                avgSimDatasets_Spearman_pvalues)

table_AvgMeasures_Spearman_SimDatasets <- mat.sort(table_AvgMeasures_Spearman_SimDatasets, ncol(table_AvgMeasures_Spearman_SimDatasets) - 1, decreasing = TRUE)

table_AvgMeasures_Pearson_SimDatasets <- cbind(table_AvgMeasures_Pearson_SimDatasets,
                                            Avg = rowMeans(table_AvgMeasures_Pearson_SimDatasets[1:nrow(table_AvgMeasures_Pearson_SimDatasets),]),
                                            avgSimDatasets_Pearson_pvalues)

table_AvgMeasures_Pearson_SimDatasets <- mat.sort(table_AvgMeasures_Pearson_SimDatasets, ncol(table_AvgMeasures_Pearson_SimDatasets) - 1, decreasing = TRUE)

# We make a copy of the tables in order to round their values to 3 decimal digits

table_AvgMeasures_Spearman_SimDatasets_rounded <- round(table_AvgMeasures_Spearman_SimDatasets, 3);
table_AvgMeasures_Pearson_SimDatasets_rounded <- round(table_AvgMeasures_Pearson_SimDatasets, 3);

# We save all final assembled data tables 

write.csv(table_AvgMeasures_Spearman_SimDatasets, file = paste(outputDir, sep="","table_AvgMeasures_Spearman_SimDatasets.csv"))
write.csv(table_AvgMeasures_Spearman_SimDatasets_rounded, file = paste(outputDir, sep="","table_AvgMeasures_Spearman_SimDatasets_rounded.csv"))

write.csv(table_AvgMeasures_Pearson_SimDatasets, file = paste(outputDir, sep="","table_AvgMeasures_Pearson_SimDatasets.csv"))
write.csv(table_AvgMeasures_Pearson_SimDatasets_rounded, file = paste(outputDir, sep="","table_AvgMeasures_Pearson_SimDatasets_rounded.csv"))

# EVALUATION OF NOUN RELATEDNESS DATASETS
#
#  We create two tables for Pearson and Spearman metrics
# by removing first two columns which contain the word pairs and human judgements. 
# Measures are arranged in rows whilst datasets are arranged in columns.
#  These two tables reproduce the same tables reporting the Pearson and
# Spearman correlation values of all measure in the five noun similarity
# datasets but using the averaged measures.
#  Input data is defined by the vectors of raw similarity values returned
# by each measure.

table_AvgMeasures_Spearman_RelDatasets <- matrix(nrow = ncol(rawdata_MC28) - 2,
                                                 ncol = length(rawdataRelNounDatasets),
                                                 dimnames = list(colnames(rawdata_MC28)[3:ncol(rawdata_MC28)],
                                                                 c("MTurk771", "MTurk287_235", "WS353Rel", "Rel122")))

table_AvgMeasures_Pearson_RelDatasets <- table_AvgMeasures_Spearman_RelDatasets

# Loop for the computation of the metrics

nMeasures <- nrow(table_AvgMeasures_Spearman_RelDatasets)
nDatasets <- length(rawdataRelNounDatasets)

# We extract the best perfomring relatedness measure (Paragram-ws)

iBestRelMeasure <- 27

for (iDataset in 1:nDatasets)
{
  # We get the raw data of the next dataset
  
  rawdata <- rawdataRelNounDatasets[[iDataset]]
  
  # We get the human judgement vector and the raw similarity values
  # of the best performing measure in the current dataset
  
  humanJugments <- rawdata[,2]
  rawbestMeasure <- rawdata[, iBestRelMeasure + 2]
  
  # We evaluate the Spearman correlation for each averaged measure in the current dataset
  
  for (iMeasure in 1:nMeasures)
  {
    # We compute the average similarity values corresponding to
    # the combination of the best performing measure with iMeasure
    
    currentMeasure <- rawdata[, iMeasure + 2]
    averaged_sim <- 0.5 * (currentMeasure + rawbestMeasure)      
    
    # We compute the Spearman correlation of the averaged measure
    
    table_AvgMeasures_Spearman_RelDatasets[iMeasure, iDataset] <- cor(humanJugments, averaged_sim, method = "spearman")
    table_AvgMeasures_Pearson_RelDatasets[iMeasure, iDataset] <- cor(humanJugments, averaged_sim, method = "pearson")
  }
}

# We compute the p-values comparing the performance of averaged measures with
# their base counterpart. The aim is to test the hypothesis that the combination
# of each measure with the best performing one outperforms the individual measure

avgRelDatasets_Pearson_pvalues <- matrix(nrow = nrow(table_AvgMeasures_Pearson_RelDatasets), ncol = 1)
colnames(avgRelDatasets_Pearson_pvalues) = "p-value"

avgRelDatasets_Spearman_pvalues <- matrix(nrow = nrow(table_AvgMeasures_Spearman_RelDatasets), ncol = 1)
colnames(avgRelDatasets_Spearman_pvalues) = "p-value"

for (iMeasure in 1:nMeasures)
{
  avgRelDatasets_Pearson_pvalues[iMeasure, 1] <- signif(t.test(table_AvgMeasures_Pearson_RelDatasets[iMeasure, ],
                                                               backup_table_Pearson_RelDatasets[iMeasure, ],
                                                               paired = TRUE,alternative="greater")$p.value,
                                                        digits=2)
  
  avgRelDatasets_Spearman_pvalues[iMeasure, 1] <- signif(t.test(table_AvgMeasures_Spearman_RelDatasets[iMeasure, ],
                                                                backup_table_Spearman_RelDatasets[iMeasure, ],
                                                                paired = TRUE,alternative="greater")$p.value,
                                                         digits=2)
}
# We compute the average values per row and sort the rows

table_AvgMeasures_Spearman_RelDatasets <- cbind(table_AvgMeasures_Spearman_RelDatasets,
                                                Avg = rowMeans(table_AvgMeasures_Spearman_RelDatasets[1:nrow(table_AvgMeasures_Spearman_RelDatasets),]),
                                                avgRelDatasets_Spearman_pvalues)

table_AvgMeasures_Spearman_RelDatasets <- mat.sort(table_AvgMeasures_Spearman_RelDatasets, ncol(table_AvgMeasures_Spearman_RelDatasets) - 1, decreasing = TRUE)

table_AvgMeasures_Pearson_RelDatasets <- cbind(table_AvgMeasures_Pearson_RelDatasets,
                                               Avg = rowMeans(table_AvgMeasures_Pearson_RelDatasets[1:nrow(table_AvgMeasures_Pearson_RelDatasets),]),
                                               avgRelDatasets_Pearson_pvalues)

table_AvgMeasures_Pearson_RelDatasets <- mat.sort(table_AvgMeasures_Pearson_RelDatasets, ncol(table_AvgMeasures_Pearson_RelDatasets) - 1, decreasing = TRUE)

# We make a copy of the tables in order to round their values to 3 decimal digits

table_AvgMeasures_Spearman_RelDatasets_rounded <- round(table_AvgMeasures_Spearman_RelDatasets, 3);
table_AvgMeasures_Pearson_RelDatasets_rounded <- round(table_AvgMeasures_Pearson_RelDatasets, 3);

# We save all final assembled data tables 

write.csv(table_AvgMeasures_Spearman_RelDatasets, file = paste(outputDir, sep="","table_AvgMeasures_Spearman_RelDatasets.csv"))
write.csv(table_AvgMeasures_Spearman_RelDatasets_rounded, file = paste(outputDir, sep="","table_AvgMeasures_Spearman_RelDatasets_rounded.csv"))

write.csv(table_AvgMeasures_Pearson_RelDatasets, file = paste(outputDir, sep="","table_AvgMeasures_Pearson_RelDatasets.csv"))
write.csv(table_AvgMeasures_Pearson_RelDatasets_rounded, file = paste(outputDir, sep="","table_AvgMeasures_Pearson_RelDatasets_rounded.csv"))

#-------------------------------
# HTML report generation
#-------------------------------

library(knitr)
library(readr)

# We load and browse Table 4 in the EAAI paper

kable_out <- kable(table_Pearson_SimDatasets_rounded,
                   caption = "Table 4: Pearson correlaton values in similarity datasets",
                   format = "html",
                   align = c('c','c','c','c','c','c','c'))

readr::write_file(kable_out, "Table_4_EAAI_paper.html")
browseURL("Table_4_EAAI_paper.html")

# We load and browse Table 5 in the EAAI paper

kable_out <- kable(table_Spearman_SimDatasets_rounded,
                   caption = "Table 5: Spearman correlaton values in similarity datasets",
                   format = "html",
                   align = c('c','c','c','c','c','c','c'))

readr::write_file(kable_out, "Table_5_EAAI_paper.html")
browseURL("Table_5_EAAI_paper.html")

# We load and browse Table 6 in the EAAI paper

kable_out <- kable(table_Pearson_RelDatasets_rounded,
                   caption = "Table 6: Pearson correlaton values in relatedness datasets",
                   format = "html",
                   align = c('c','c','c','c','c','c'))

readr::write_file(kable_out, "Table_6_EAAI_paper.html")
browseURL("Table_6_EAAI_paper.html")

# We load and browse Table 7 in the EAAI paper

kable_out <- kable(table_Spearman_RelDatasets_rounded,
                   caption = "Table 7: Spearman correlaton values in relatedness datasets",
                   format = "html",
                   align = c('c','c','c','c','c','c'))

readr::write_file(kable_out, "Table_7_EAAI_paper.html")
browseURL("Table_7_EAAI_paper.html")

# We load and browse Table 8 in the EAAI paper

kable_out <- kable(table_joined_allEmbeddings_similarity_rounded,
                   caption = "Table 8: Pearson, Spearman and Harmonic score metrics ontained by WE models in all similarity datasets",
                   format = "html",
                   align = c('l','c','c','c','c','c','c','c','c','c','c','c'))

readr::write_file(kable_out, "Table_8_EAAI_paper.html")
browseURL("Table_8_EAAI_paper.html")

# We load and browse Table 9 in the EAAI paper

kable_out <- kable(table_joined_allEmbeddings_relatedness_rounded,
                   caption = "Table 9: Pearson, Spearman and Harmonic score metrics ontained by WE models in all relatedness datasets",
                   format = "html",
                   align = c('l','c','c','c','c','c','c','c','c','c','c','c'))

readr::write_file(kable_out, "Table_9_EAAI_paper.html")
browseURL("Table_9_EAAI_paper.html")

# We load and browse Table A.1 in the EAAI paper

kable_out <- kable(table_pvalues_AttractReppel_allembeddings_similarity,
                   caption = "Table A.1: P-values comparing the Attract-repel model with the rest of WE and OVM models in all similarity datasets",
                   format = "html",
                   align = c('l','l','l','l','l','l'))

readr::write_file(kable_out, "Table_A1_EAAI_paper.html")
browseURL("Table_A1_EAAI_paper.html")

# We load and browse Table A.2 in the EAAI paper

kable_out <- kable(table_pvalues_Paragramws_allembeddings_relatedness,
                   caption = "Table A.2: P-values comparing the Paragram-ws model with the rest of WE and OVM models in all relatedness datasets",
                   format = "html",
                   align = c('l','l','l','l','l','l'))

readr::write_file(kable_out, "Table_A2_EAAI_paper.html")
browseURL("Table_A2_EAAI_paper.html")

# We load and browse Table A.3 in the EAAI paper

kable_out <- kable(table_AvgMeasures_Pearson_SimDatasets_rounded,
                   caption = "Table A.3: Pearson correlation (r) values for the combined measures defined by the arithmetic mean of the similarity values returned by the Attract-repel model and each remaining base measure.",
                   format = "html",
                   align = c('l','c','c','c','c','c','c','c'))

readr::write_file(kable_out, "Table_A3_EAAI_paper.html")
browseURL("Table_A3_EAAI_paper.html")

# We load and browse Table A.4 in the EAAI paper

kable_out <- kable(table_AvgMeasures_Spearman_SimDatasets_rounded,
                   caption = "Table A.4: Spearman rank correlation values for the combined measures defined by the arithmetic mean of the similarity values returned by the Attract-repel model and each remaining base measure.",
                   format = "html",
                   align = c('l','c','c','c','c','c','c','c'))

readr::write_file(kable_out, "Table_A4_EAAI_paper.html")
browseURL("Table_A4_EAAI_paper.html")

# We load and browse Table A.5 in the EAAI paper

kable_out <- kable(table_AvgMeasures_Pearson_RelDatasets_rounded,
                   caption = "Table A.5: Pearson correlation values for the combined measures defined by the arithmetic mean of the similarity values returned by the Paragram-ws model and each remaining base measure.",
                   format = "html",
                   align = c('l','c','c','c','c','c','c'))

readr::write_file(kable_out, "Table_A5_EAAI_paper.html")
browseURL("Table_A5_EAAI_paper.html")

# We load and browse Table A.6 in the EAAI paper

kable_out <- kable(table_AvgMeasures_Spearman_RelDatasets_rounded,
                   caption = "Table A.6: Spearman rank correlation values for the combined measures defined by the arithmetic mean of the similarity values returned by the Paragram-ws model and each remaining base measure.",
                   format = "html",
                   align = c('l','c','c','c','c','c','c'))

readr::write_file(kable_out, "Table_A6_EAAI_paper.html")
browseURL("Table_A6_EAAI_paper.html")
