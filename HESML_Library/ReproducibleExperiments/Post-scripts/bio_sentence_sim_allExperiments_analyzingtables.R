# Description:
#
# This script loads a collection of sentence similarity benchmarks generated
# by HESMLSTSclient program, which contain the raw similarity values for
# sentence pair. Then, the script computes a collection of consolidated tables
# including the Pearson, Spearman and harmonic score metrics.
#
# References:
# ----------

# We clear all session variables

rm(list = ls())

# IMPORTANT:configuration of the input/output directories
# We define below the input directory for the input raw results
# in CSV file format, and the output directory for the
# final assembled tables in CSV file format.
# You must change these values in order to
# point to the proper directories in your hard drive.
# We also define below the name of the input raw CSV files
# containing the experimental results.

# The input and output directories below must end with '/' in
# Unix-like format to be compatible with Windows
# or Linux-based R distributions.

# First, we get the current file script directory

current_working_dir <- dirname(rstudioapi::getActiveDocumentContext()$path)

# and we set the working directory for the R project

setwd(current_working_dir)

# Define the root path 

rootDir = "../BioSentenceSimilarity_paper"

# We import the library that implements the data structures.

library(collections)

# We create an OrderedDict object with all the experiments that will be evaluated.

experiments <- ordered_dict()

# We define the subdirectory experiment and the caption for showing the results in Latex and HTML format
# and we add to the dictionary all the String-based measures experiments

experimentSubdirectory = "BioSentenceSimFinalRawOutputFiles"
preprocessedExperimentsSubdirectory = "BioSentenceSimFinalProcessedOutputFiles"

# We define the output directory 

dir.create(paste(rootDir, "/", preprocessedExperimentsSubdirectory, "/", sep = ""))
outputDir =  paste(rootDir, "/", preprocessedExperimentsSubdirectory, "/", sep = "")

# Input raw CSV files generated by the reproducible experiments detailed
# in the companion paper for each sentence similarity dataset

# We define the input datasets directories

inputDir = paste(rootDir, "/", experimentSubdirectory, "/", sep = "")

# We load the input raw results file for best combinations (FINAL RESULTS)

source(paste("bio_sentence_sim_scripts", "readBESTCOMBS.R", sep = "/"), local = knitr::knit_global())

# We initialize the counter of executions

counter_executions <- 0

# We get the label

experimentLabel = rawdata_BESTCOMBS$label

# We get the experiment subdirectory and the caption for the Latex File

strCaption = paste("Table \\label{table:", experimentSubdirectory, "}: Pearson (r), Spearman ($\rho$) and Harmonic score (h) obtained by each unsupervised ", experimentLabel, " similarity method evaluated herein.", sep="")

rawdata_BIOSSES <- rawdata_BESTCOMBS$biosses
rawdata_MedSTS  <- rawdata_BESTCOMBS$medsts
rawdata_CTR     <- rawdata_BESTCOMBS$ctr

# mat.sort function is copied from source files of
# BioPhysConnectoR package which is now unavailable.
# Source code was retrieved from https://rdrr.io/cran/BioPhysConnectoR/src/R/mat.sort.r

mat.sort<-function(mat,sort,decreasing=FALSE)
{
  m<-do.call("order",c(as.data.frame(mat[,sort]),decreasing=decreasing))
  mat[m,]
}

# ---------------------------------------------------------------------
# Raw output file format:
# Raw similarity files contain the similarity values returned by each
# semantic measure for each sentence pair. 
# First column contains the uman judgements for each sentence pair
# whilst subsequent columns contain the values returned by the
# sentence similaritu measures.
# ---------------------------------------------------------------------

# ---------------------------------------------------------------------
# Table 1: Pearson and Spearman metrics of all sentence similarity
# measures.
# ---------------------------------------------------------------------

# We split the MedSTS dataset into 10 subdataset to artificially increase the sample size
# from 3 datasets to 12 datasets.

# We calculate the total of rows per dataset

number_samples_per_subdataset <- nrow(rawdata_MedSTS) / 10

# We truncate the value 

number_samples_per_subdataset_truncated <- trunc(number_samples_per_subdataset, 0)

# We split the dataset into 10 datasets of 106 rows (except for the last one, with XX rows)

init_value_split1 <- 1
end_value_split1 <- number_samples_per_subdataset_truncated
rawdata_MedSTS_split1 <- rawdata_MedSTS[init_value_split1:end_value_split1,]

init_value_split2 <- init_value_split1+number_samples_per_subdataset_truncated
end_value_split2 <- init_value_split2+number_samples_per_subdataset_truncated
rawdata_MedSTS_split2 <- rawdata_MedSTS[init_value_split2:end_value_split2,]

init_value_split3 <- init_value_split2+number_samples_per_subdataset_truncated+1
end_value_split3 <- init_value_split3+number_samples_per_subdataset_truncated
rawdata_MedSTS_split3 <- rawdata_MedSTS[init_value_split3:end_value_split3,]

init_value_split4 <- init_value_split3+number_samples_per_subdataset_truncated+1
end_value_split4 <- init_value_split4+number_samples_per_subdataset_truncated
rawdata_MedSTS_split4 <- rawdata_MedSTS[init_value_split4:end_value_split4,]

init_value_split5 <- init_value_split4+number_samples_per_subdataset_truncated+1
end_value_split5 <- init_value_split5+number_samples_per_subdataset_truncated
rawdata_MedSTS_split5 <- rawdata_MedSTS[init_value_split5:end_value_split5,]

init_value_split6 <- init_value_split5+number_samples_per_subdataset_truncated+1
end_value_split6 <- init_value_split6+number_samples_per_subdataset_truncated
rawdata_MedSTS_split6 <- rawdata_MedSTS[init_value_split6:end_value_split6,]

init_value_split7 <- init_value_split6+number_samples_per_subdataset_truncated+1
end_value_split7 <- init_value_split7+number_samples_per_subdataset_truncated
rawdata_MedSTS_split7 <- rawdata_MedSTS[init_value_split7:end_value_split7,]

init_value_split8 <- init_value_split7+number_samples_per_subdataset_truncated+1
end_value_split8 <- init_value_split8+number_samples_per_subdataset_truncated
rawdata_MedSTS_split8 <- rawdata_MedSTS[init_value_split8:end_value_split8,]

init_value_split9 <- init_value_split8+number_samples_per_subdataset_truncated+1
end_value_split9 <- init_value_split9+number_samples_per_subdataset_truncated
rawdata_MedSTS_split9 <- rawdata_MedSTS[init_value_split9:end_value_split9,]

# The last dataset has the remaining rows

init_value_split10 <- init_value_split9+number_samples_per_subdataset_truncated+1
end_value_split10 <- nrow(rawdata_MedSTS)
rawdata_MedSTS_split10 <- rawdata_MedSTS[init_value_split10:end_value_split10,]


# We define all datasets

# rawdata_allDatasets <- list(rawdata_BIOSSES, rawdata_MedSTS, rawdata_CTR)

rawdata_allDatasets <- list(rawdata_BIOSSES, rawdata_CTR, rawdata_MedSTS_split1,
                            rawdata_MedSTS_split2, rawdata_MedSTS_split3,
                            rawdata_MedSTS_split4, rawdata_MedSTS_split5,
                            rawdata_MedSTS_split6, rawdata_MedSTS_split7, 
                            rawdata_MedSTS_split8, rawdata_MedSTS_split9, 
                            rawdata_MedSTS_split10)


# We create a separated table for each metric by removing the first column
# which contains the human judgements. Measures are arranged in rows
# whilst datasets are arranged in columns.

table_Pearson <- matrix(nrow = ncol(rawdata_BIOSSES) - 1,
                        ncol = length(rawdata_allDatasets),
                        dimnames = list(colnames(rawdata_BIOSSES)[2:ncol(rawdata_BIOSSES)],
                                        c("BIOSSES", "CTR", 
                                          "MedSTS_split1",
                                          "MedSTS_split2",
                                          "MedSTS_split3",
                                          "MedSTS_split4",
                                          "MedSTS_split5",
                                          "MedSTS_split6",
                                          "MedSTS_split7",
                                          "MedSTS_split8",
                                          "MedSTS_split9",
                                          "MedSTS_split10")))

table_Spearman <- table_Pearson
table_Harmonic <- table_Pearson

# Loop for the computation of the metrics

nMeasures <- nrow(table_Pearson)
nDatasets <- length(rawdata_allDatasets)

for (iDataset in 1:nDatasets)
{
  # We get the raw data of the next dataset
  
  rawdata <- rawdata_allDatasets[[iDataset]]
  
  # We evaluate the Pearson and Spearman metrics for each measure in the current dataset
  
  for (iMeasure in 1:nMeasures)
  {
    table_Pearson[iMeasure, iDataset] <- cor(rawdata[,1], rawdata[, iMeasure + 1], use="complete.obs", method = "pearson")
    table_Spearman[iMeasure, iDataset] <- cor(rawdata[,1], rawdata[, iMeasure + 1], use="complete.obs", method = "spearman")
    table_Harmonic[iMeasure, iDataset] <- 2 * table_Pearson[iMeasure, iDataset] * table_Spearman[iMeasure, iDataset] / (table_Pearson[iMeasure, iDataset] + table_Spearman[iMeasure, iDataset])
  }
}

# ------------------------------------------------------------
# We merge all metrics (Pearson, Spearman and Harmonic score)
# into a same data table by concatening previouos data tables.
# ------------------------------------------------------------

table_allMetrics <- cbind(table_Pearson, table_Spearman, table_Harmonic, Avg = rowMeans(table_Harmonic[1:nrow(table_Harmonic),]))

# table_allMetrics <- cbind(table_Pearson, table_Spearman, Avg = rowMeans(table_Harmonic[1:nrow(table_Harmonic),]))


table_allMetrics <- mat.sort(table_allMetrics, ncol(table_allMetrics), decreasing = TRUE)

ncol(table_allMetrics)

# We set the column names to the final matrix

colnames(table_allMetrics) <- c("r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r", "r",
                                '$\\rho$', '$\\rho$', '$\\rho$', '$\\rho$', '$\\rho$', '$\\rho$', '$\\rho$', '$\\rho$', '$\\rho$', '$\\rho$', '$\\rho$', '$\\rho$',
                                "h", "h", "h", "h", "h", "h", "h", "h", "h", "h", "h", "h",
                                "Avg")

# We reorder the matrix columns grouping by dataset names.

# table_allMetrics <- table_allMetrics[ , c(1,4,7,2,5,8,3,6,9,10) ]

# This script computes a matrix of pairwise p-values between
# a set of methods defined by row vectors. 
# We read the raw table generated by HESMLSTSclient
# and compute the p-values

# We get the dimensions of the matrix

rowBasedMatrixMethods <- table_allMetrics[,25:36]
rowNames <- rownames(table_allMetrics)

# We get the dimensions of the matrix

nrows = nrow(rowBasedMatrixMethods)
ncols = ncol(rowBasedMatrixMethods)

# We create a blank square matrix to store the pairwise
# p-values between two paired rows (metrod samples)

outputMatrix = matrix(nrow=nrows,ncol=nrows)

# We compute the p-values between pairwise methods


for (i in 1:(nrows - 1))
{
  for (j in (i + 1):nrows)
  {
    val <- as.numeric(matrix(rowBasedMatrixMethods[i,1:ncols]))
    val
    outputMatrix[i,j] = t.test(x = as.numeric(matrix(rowBasedMatrixMethods[i,1:ncols])), 
                               y = as.numeric(matrix(rowBasedMatrixMethods[j,1:ncols])), alternative = "two.sided", 
                               mu = 0, paired = TRUE, conf.level = 0.95)$p.value * 0.5
    outputMatrix[j,i] = outputMatrix[i,j]
  }
}

# We set the names of rows and columns

colnames(outputMatrix) = rowNames
rownames(outputMatrix) = rowNames

final_data <- round(outputMatrix, 3)

write.csv(final_data, file = paste(outputDir, sep="","Pvalues.csv"))

rowNames

# [1] "Jaccard"                                                                                              
# [2] "BlockDistance"                                                                                        
# [3] "Levenshtein"                                                                                          
# [4] "OverlapCoefficient"                                                                                   
# [5] "Qgram"                                                                                                
# [6] "bioc_skipgram_defaultchar_Min"                                                                        
# [7] "bioconceptvec_fasttext"                                                                               
# [8] "bioconceptvec_glove"                                                                                  
# [9] "bioconceptvec_word2vec_cbow"                                                                          
# [10] "bioconceptvec_word2vec_skipgram"                                                                      
# [11] "PubMed_CBOW"                                                                                          
# [12] "PubMed_Glove"                                                                                         
# [13] "PubMed_SkipGramNegSampling"                                                                           
# [14] "PubMed.and.PMC.w2v"                                                                                   
# [15] "bio_embedding_extrinsic"                                                                              
# [16] "bio_embedding_intrinsic"                                                                              
# [17] "BioNLP2016_PubMed.shuffle.win.2"                                                                      
# [18] "BioNLP2016_PubMed.shuffle.win.30"                                                                     
# [19] "WBSM_AncSPLWeightedJiangConrath"                                                                      
# [20] "WBSM_AncSPLRada"                                                                                      
# [21] "WBSM_AncSPLCosineNormWeightedJiangConrath"                                                            
# [22] "WBSM_AncSPLCaiStrategy1"                                                                              
# [23] "WBSM_JiangConrath"                                                                                    
# [24] "oubiobert.base.uncased_tok.wordpiecetokenizer_lc_sw.user_cf.default_ner.none"                         
# [25] "scibert_scivocab_uncased_tok.wordpiecetokenizer_lc_sw.user_cf.biosses_ner.none"                       
# [26] "pubmedbert.base.uncased.abstract_tok.wordpiecetokenizer_lc_sw.user_cf.default_ner.none"               
# [27] "pubmedbert.base.uncased.abstract.fulltext_tok.wordpiecetokenizer_lc_sw.user_cf.default_ner.none"      
# [28] "biobert_v1.0_pubmed_tok.wordpiecetokenizer_lc_sw.user_cf.biosses_ner.none"                            
# [29] "biobert_v1.0_pmc_tok.wordpiecetokenizer_lc_sw.user_cf.biosses_ner.none"                               
# [30] "biobert_v1.0_pubmed_pmc_tok.wordpiecetokenizer_lc_sw.user_cf.biosses_ner.none"                        
# [31] "ncbi_bert_pubmed_mimic_uncased_l.12_h.768_a.12_tok.wordpiecetokenizer_lc_sw.user_cf.biosses_ner.none" 
# [32] "ncbi_bert_pubmed_mimic_uncased_l.24_h.1024_a.16_tok.wordpiecetokenizer_lc_sw.user_cf.biosses_ner.none"
# [33] "ncbi_bert_pubmed_uncased_l.12_h.768_a.12_tok.wordpiecetokenizer_lc_sw.user_cf.blagec2019_ner.none"    
# [34] "ncbi_bert_pubmed_uncased_l.24_h.1024_a.16_tok.wordpiecetokenizer_lc_sw.user_cf.biosses_ner.none"      
# [35] "bio.clinicalbert_tok.wordpiecetokenizer_lc_sw.user_cf.blagec2019_ner.none"                            
# [36] "bio.dischargesummarybert_tok.wordpiecetokenizer_lc_sw.user_cf.blagec2019_ner.none"                    
# [37] "clinicalbert_tok.wordpiecetokenizer_notlc_sw.user_cf.blagec2019_ner.none"                             
# [38] "dischargesummarybert_tok.wordpiecetokenizer_notlc_sw.user_cf.blagec2019_ner.none"                     
# [39] "biobert_v1.1_pubmed_tok.wordpiecetokenizer_notlc_sw.user_cf.blagec2019_ner.none"                      
# [40] "biobert_large_v1.1_pubmed_tok.wordpiecetokenizer_notlc_sw.user_cf.blagec2019_ner.none"                
# [41] "Sent2vec_BioSentVec_PubMed_MIMICIII.bigram_d700"                                                      
# [42] "USE"                                                                                                  
# [43] "Flair"                                                                                                
# [44] "UBSM_AncSPLWeightedJiangConrath_tok.stanfordcorenlpv4_2_0_lc_sw.user_cf.biosses_ner.ctakes"           
# [45] "UBSM_AncSPLRada_tok.stanfordcorenlpv4_2_0_lc_sw.user_cf.biosses_ner.ctakes"                           
# [46] "UBSM_AncSPLCosineNormWeightedJiangConrath_tok.stanfordcorenlpv4_2_0_lc_sw.user_cf.biosses_ner.ctakes" 
# [47] "UBSM_AncSPLCaiStrategy1_tok.stanfordcorenlpv4_2_0_lc_sw.user_cf.biosses_ner.ctakes"                   
# [48] "UBSM_JiangConrath_tok.stanfordcorenlpv4_2_0_lc_sw.user_cf.biosses_ner.ctakes"                         
# [49] "COM_WBSM_AncSPLRada_UBSM_AncSPLWeightedJiangConrath"                                                  
# [50] "COMMixed_LietalBase.AncSPLWeightedJiangConrath_String.BlockDistance_lambda0.5"


table_allMetrics[41, 7:9]

table_allMetrics[41, 7:9]

pvalues_r_harmonic <- signif(t.test(table_allMetrics[2, 7:9],
                                    table_allMetrics[4, 7:9], 
                                    paired = TRUE,alternative="greater", 
                                    var.equal=TRUE)$p.value, 
                             digits=3)
pvalues_r_harmonic

pvalues_r_spearman <- signif(t.test(table_allMetrics[41, 4:6],
                                    table_allMetrics[43, 4:6], 
                                    paired = TRUE,alternative="greater")$p.value, digits=3)
pvalues_r_spearman

pvalues_r_pearson <- signif(t.test(table_allMetrics[41, 1:3],
                                   table_allMetrics[43, 1:3], 
                                   paired = TRUE,alternative="greater")$p.value, digits=3)
pvalues_r_pearson

